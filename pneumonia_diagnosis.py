# -*- coding: utf-8 -*-
"""pneumonia-diagnosis (3) (1) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11SIwF84iqWHqZBhiDNEWe8pFuVJwfkQx

# Pneumonia Diagnosis using Deep learning
"""



"""Diagnosis of pneumonia has been a major challenge in the medical field. So given a Chest X-ray Image of the patient try to come up with a computer-aided diagnosis  system to guide the clinicians for predicting if the patient is suffering from pneumonia or if he/she is normal.

### Importing Libraries
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
import cv2
from PIL import Image
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization, Reshape, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from keras.layers import GlobalAveragePooling2D
from keras.applications import VGG16,InceptionResNetV2,ResNet50,Xception
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau
from keras.regularizers import l2
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score

"""### Checking Directories"""

os.listdir("chest_xray")

"""### Plotting images from pneumonia directory"""

pneumonia = os.listdir("chest_xray/train/PNEUMONIA")
pneumonia_dir = "chest_xray/train/PNEUMONIA"

plt.figure(figsize=(20, 10))

for i in range(12):
    plt.subplot(4, 3, i + 1)
    img = plt.imread(os.path.join(pneumonia_dir, pneumonia[i]))
    plt.imshow(img, cmap='gray')
    plt.axis('off')

plt.tight_layout()

"""### Plotting images from normal directory"""

normal = os.listdir("chest_xray/train/NORMAL")
normal_dir = "chest_xray/train/NORMAL"

plt.figure(figsize=(20, 10))

for i in range(12):
    plt.subplot(4, 3, i + 1)
    img = plt.imread(os.path.join(normal_dir, normal[i]))
    plt.imshow(img, cmap='gray')
    plt.axis('off')

plt.tight_layout()

"""### Printing number of pneumonia and normal images in each training, testing and validation directories"""

train_dir = "chest_xray/train"
test_dir = "chest_xray/test"
val_dir = "chest_xray/val"

print("Training set:")
pneumonia_len = len(os.listdir(os.path.join(train_dir, 'PNEUMONIA')))
normal_len = len(os.listdir(os.path.join(train_dir, 'NORMAL')))
print(f"PNEUMONIA={pneumonia_len}")
print(f"NORMAL={normal_len}")
print("==========================================")
print("Testing set:")
print(f"PNEUMONIA={len(os.listdir(os.path.join(test_dir, 'PNEUMONIA')))}")
print(f"NORMAL={len(os.listdir(os.path.join(test_dir, 'NORMAL')))}")
print("==========================================")
print("Validation set:")
print(f"PNEUMONIA={len(os.listdir(os.path.join(val_dir, 'PNEUMONIA')))}")
print(f"NORMAL={len(os.listdir(os.path.join(val_dir, 'NORMAL')))}")

"""## Loading Images and applying Histogram Equalization on Images"""

labels = ['NORMAL','PNEUMONIA']
folders=['train','test','val']
def load_images(main_dir,foldername):
    total_labels=[]
    images=[]
    total_normal=0
    total_pneumonia=0
    path = os.path.join(main_dir,foldername)
    for lab in labels :
        full_path = os.path.join(path,lab)
        print ('loading ....... images from folder :',foldername+'/'+lab )
        for image in os.listdir(full_path):
            img = cv2.imread(full_path+'/'+image)
            img = cv2.resize(img,(224,224))
            new_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            new_img = cv2.equalizeHist(new_img)
            new_img = cv2.cvtColor(new_img,cv2.COLOR_GRAY2RGB)
            images.append(new_img)
            if lab  == 'NORMAL':
                    label =0
                    total_normal+= 1
            elif lab == 'PNEUMONIA' :
                    label = 1
                    total_pneumonia +=1
            total_labels.append(label)
    print('total normal image := ',total_normal)
    print('total Pneumonia    := ',total_pneumonia)
    return images,total_labels

train_images,train_labels=load_images("chest_xray","train")
print("=============================")
test_images,test_labels=load_images("chest_xray","test")
print("=============================")
val_images,val_labels=load_images("chest_xray","val")

"""### Converting images to numpy array"""

train_images  = np.asarray(train_images,np.float32)/255
train_labels = np.asarray(train_labels)
test_images  = np.asarray(test_images,np.float32)/255
test_labels = np.asarray(test_labels)
val_images  = np.asarray(test_images,np.float32)/255
val_labels = np.asarray(test_labels)

"""### Data Augmentation on images"""

batch_size = 32
image_gen = ImageDataGenerator(
        rotation_range = 30,
        shear_range = 0.2,
        zoom_range = 0.2,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip = True,
        )

test_data_gen = ImageDataGenerator()
val_data_gen  = ImageDataGenerator()

train = image_gen.flow(
      train_images,
      train_labels,
      shuffle=True,
      batch_size=batch_size
      )
test = test_data_gen.flow(
      test_images,
      test_labels,
      shuffle=True,
      batch_size=batch_size
      )
val = val_data_gen.flow(
      val_images,
      val_labels,
      shuffle=True,
      batch_size=batch_size
      )

"""### CNN Model"""

cnn_model = Sequential()
cnn_model = models.Sequential()
cnn_model.add(Conv2D(32,(3,3),padding ='Same',activation = 'relu',input_shape=(224,224,3)))
cnn_model.add(BatchNormalization())
cnn_model.add(MaxPooling2D(2,2))
cnn_model.add(Conv2D(64,(3,3),padding ='same',activation='relu'))
cnn_model.add(BatchNormalization())
cnn_model.add(MaxPooling2D(2,2))
cnn_model.add(Conv2D(128,(3,3),padding ='same',activation='relu'))
cnn_model.add(BatchNormalization())
cnn_model.add(MaxPooling2D(2,2))
cnn_model.add(Conv2D(256,(3,3),padding ='same',activation='relu'))
cnn_model.add(BatchNormalization())
cnn_model.add(MaxPooling2D(2,2))
cnn_model.add(Flatten())
cnn_model.add(Dense(64, activation='relu'))
cnn_model.add(Dropout(0.2))
cnn_model.add(Dense(1, activation ='sigmoid'))
cnn_model.summary()

tf.keras.utils.plot_model(cnn_model, to_file='model_plot1.png', show_shapes=True, show_layer_names=True)

"""### Compiling CNN model"""

cnn_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

"""### Declaring callbacks"""

learning_rate_reduction = ReduceLROnPlateau(monitor='loss', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)
callbacks_list = [ learning_rate_reduction]

n_validation_samples = len(val)
cnn_history = cnn_model.fit(
    train,
    epochs=100,
    validation_data=val,
    validation_steps=n_validation_samples//batch_size,
    shuffle = True,
    callbacks=callbacks_list
    )

cnn_score, cnn_acc = cnn_model.evaluate(test, batch_size=batch_size)
print('Test score:', cnn_score)
print('Test accuracy:', cnn_acc)

plt.plot(cnn_history.history['accuracy'], label='accuracy')
plt.plot(cnn_history.history['val_accuracy'], label = 'val_accuracy')
plt.title('CNN Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

plt.plot(cnn_history.history['loss'],label="loss")
plt.plot(cnn_history.history['val_loss'],label='val_loss')
plt.title('CNN loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(loc='lower right')
plt.show()

"""### VGG16 Model"""

vgg_base_model = VGG16(input_shape=(224,224,3),weights='imagenet', include_top=False)

def freezing_layers(model_name):
    for layer in model_name.layers:
        layer.trainable = False

freezing_layers(vgg_base_model)

vgg_model = Sequential()
vgg_model.add(vgg_base_model)
vgg_model.add(layers.Flatten())
vgg_model.add(Dense(128, activation = "relu"))
vgg_model.add(Dense(64, activation = "relu"))
vgg_model.add(Dense(32, activation = "relu"))
vgg_model.add(Dense(1, activation ='sigmoid'))
vgg_model.summary()

import pydot
import pydotplus
import graphviz
tf.keras.utils.plot_model(vgg_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

vgg_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

n_validation_samples = len(val)
vgg_history = vgg_model.fit(
    train,
    epochs=100,
    validation_data=val,
    validation_steps=n_validation_samples//batch_size,
    shuffle = True,
    callbacks=callbacks_list
    )

vgg_score, vgg_acc = vgg_model.evaluate(test, batch_size=batch_size)
print('Test score:', vgg_score)
print('Test accuracy:', vgg_acc)

plt.plot(vgg_history.history['accuracy'], label='accuracy')
plt.plot(vgg_history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.title('VGG16 Accuracy')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

plt.plot(vgg_history.history['loss'],label="loss")
plt.plot(vgg_history.history['val_loss'],label='val_loss')
plt.title('VGG16 loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(loc='lower right')
plt.show()

"""### InceptionResNetV2 Model"""

inception_base_model = InceptionResNetV2(include_top=False, weights="imagenet", input_shape=(224,224,3), pooling="avg")
inception_base_model.summary()

freezing_layers(inception_base_model)

inception = Sequential()
inception.add(inception_base_model)
inception.add(layers.Flatten())
inception.add(layers.Dense(2048 ,activation='relu'))
inception.add(BatchNormalization())
inception.add(Dropout(0.5))
inception.add(layers.Dense(1, activation ='sigmoid'))
inception.summary()

tf.keras.utils.plot_model(inception, to_file='model_plot1.png', show_shapes=True, show_layer_names=True)

inception.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

n_validation_samples = len(val)
inception_history = inception.fit(
    train,
    epochs=100,
    validation_data=val,
    validation_steps=n_validation_samples//batch_size,
    shuffle = True,
    callbacks=callbacks_list
    )

inc_score, inc_acc = inception.evaluate(test,batch_size=batch_size)

print('Test score:', inc_score)
print('Test accuracy:', inc_acc)

plt.plot(inception_history.history['accuracy'], label='accuracy')
plt.plot(inception_history.history['val_accuracy'], label = 'val_accuracy')
plt.title('Inception Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

plt.plot(inception_history.history['loss'],label="loss")
plt.plot(inception_history.history['val_loss'],label='val_loss')
plt.title('Inception loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(loc='lower right')
plt.show()

"""### Xception Model"""

xception_base_model = Xception(include_top=False,weights='imagenet',input_shape = (224,224,3))

freezing_layers(xception_base_model)

excmodel = Sequential()
excmodel.add(xception_base_model)
excmodel.add(layers.Flatten())
excmodel.add(layers.Dense(1024 ,activation='relu'))
excmodel.add(BatchNormalization())
excmodel.add(Dropout(0.5))
excmodel.add(layers.Dense(1, activation ='sigmoid'))
excmodel.summary()

tf.keras.utils.plot_model(excmodel, to_file='model_plot1.png', show_shapes=True, show_layer_names=True)

excmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

n_validation_samples = len(val)
exc_history = excmodel.fit(
    train,
    epochs=100,
    validation_data=val,
    validation_steps=n_validation_samples//batch_size,
    shuffle = True,
    callbacks=callbacks_list
    )

exc_score, exc_acc = excmodel.evaluate(test,batch_size=batch_size)
print('Test score:', exc_score)
print('Test accuracy:', exc_acc)

plt.plot(exc_history.history['accuracy'], label='accuracy')
plt.plot(exc_history.history['val_accuracy'], label = 'val_accuracy')
plt.title("Xception")
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

plt.plot(exc_history.history['loss'],label="loss")
plt.plot(exc_history.history['val_loss'],label='val_loss')
plt.title('Xception loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(loc='lower right')
plt.show()

"""### ResNet50 Model"""

resnet50_base_model = ResNet50(include_top=False, weights="imagenet",input_shape=(224,224,3))

freezing_layers(resnet50_base_model)
resnet = Sequential()
resnet.add(resnet50_base_model)
resnet.add(layers.GlobalAveragePooling2D())
resnet.add(Dense(1024, activation="relu"))
resnet.add(BatchNormalization())
resnet.add(Dropout(0.5))
resnet.add(Dense(1,activation="sigmoid"))


resnet.summary()

tf.keras.utils.plot_model(resnet, to_file='model_plot1.png', show_shapes=True, show_layer_names=True)

resnet.compile(  loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

n_validation_samples = len(val)
resnet_history = resnet.fit(
    train,
    epochs=100,
    validation_data=val,
    validation_steps=n_validation_samples//batch_size,
    shuffle = True,
    callbacks=callbacks_list
    )

res_score, res_acc = resnet.evaluate(test,batch_size=batch_size)

print('Test score:', res_score)
print('Test accuracy:', res_acc)

plt.plot(resnet_history.history['accuracy'], label='accuracy')
plt.plot(resnet_history.history['val_accuracy'], label = 'val_accuracy')
plt.title('Resnet50 Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

plt.plot(resnet_history.history['loss'],label="loss")
plt.plot(resnet_history.history['val_loss'],label='val_loss')
plt.title('Resnet50 loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(loc='lower right')
plt.show()

"""### Hybrid Model on Histogram Images"""

from keras.layers import Concatenate, Dense, Input, Flatten
from keras.models import Model

# define input shape for the images
input_shape = (224, 224, 3)

# define the two pre-trained models
vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)
resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

# set the layers of the models to be non-trainable
for layer in vgg16.layers:
    layer.trainable = False
for layer in resnet50.layers:
    layer.trainable = False

# define the input layer
input_layer = Input(shape=input_shape)

# get the output from each model
vgg16_output = vgg16(input_layer)
resnet50_output = resnet50(input_layer)

# concatenate the outputs
concatenated = Concatenate()([vgg16_output, resnet50_output])

# flatten the concatenated output
flattened = Flatten()(concatenated)

# add dense layers for classification
dense1 = Dense(512, activation='relu')(flattened)
dense2 = Dense(256, activation='relu')(dense1)
output_layer = Dense(1, activation='sigmoid')(dense2)

# define the model
hybrid_model = Model(inputs=input_layer, outputs=output_layer)

# compile the model
hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
hybrid_model.summary()

hybrid_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

n_validation_samples = len(val)
hybrid_history = hybrid_model.fit(
    train,
    epochs=100,
    validation_data=val,
    shuffle = True,
    validation_steps=n_validation_samples//batch_size,
    callbacks=callbacks_list
    )

hybrid_score, hybrid_acc = hybrid_model.evaluate(test,batch_size=batch_size)

print('Test score:', hybrid_score)
print('Test accuracy:', hybrid_acc)

plt.plot(hybrid_history.history['accuracy'], label='accuracy')
plt.plot(hybrid_history.history['val_accuracy'], label = 'val_accuracy')
plt.title('Hybrid Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

plt.plot(hybrid_history.history['loss'],label="loss")
plt.plot(hybrid_history.history['val_loss'],label='val_loss')
plt.title('Hybrid Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(loc='lower right')
plt.show()

from keras.layers import Concatenate, Dense, Input, Flatten
from keras.models import Model

# define input shape for the images
input_shape = (224, 224, 3)

# define the two pre-trained models
vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)
resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

# set the layers of the models to be non-trainable
for layer in vgg16.layers[-2:]:
    layer.trainable = False
for layer in resnet50.layers[-18:]:
    layer.trainable = False

# define the input layer
input_layer = Input(shape=input_shape)

# get the output from each model
vgg16_output = vgg16(input_layer)
resnet50_output = resnet50(input_layer)

# concatenate the outputs
concatenated = Concatenate()([vgg16_output, resnet50_output])

# flatten the concatenated output
flattened = Flatten()(concatenated)

# add dense layers for classification
dense1 = Dense(512, activation='relu')(flattened)
dense2 = Dense(256, activation='relu')(dense1)
output_layer = Dense(1, activation='sigmoid')(dense2)

# define the model
hybrid_model = Model(inputs=input_layer, outputs=output_layer)

# compile the model
hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

hybrid_model.summary()

hybrid_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

n_validation_samples = len(val)
hybrid_history = hybrid_model.fit(
    train,
    epochs=100,
    validation_data=val,
    shuffle = True,
    validation_steps=n_validation_samples//batch_size,
    callbacks=callbacks_list
    )

hybrid_score, hybrid_acc = hybrid_model.evaluate(test,batch_size=batch_size)

print('Test score:', hybrid_score)
print('Test accuracy:', hybrid_acc)